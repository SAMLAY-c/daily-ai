import requests
import json
from bs4 import BeautifulSoup
import time
import os
from datetime import datetime


def get_article_content(url, max_retries=3):
    """è·å–å¾®ä¿¡æ–‡ç« çš„æ–‡å­—å†…å®¹"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

    for attempt in range(max_retries):
        try:
            print(f"  æ­£åœ¨è·å–æ–‡ç« å†…å®¹ (å°è¯• {attempt + 1}/{max_retries})...")
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            # ä½¿ç”¨ BeautifulSoup è§£æ HTML
            soup = BeautifulSoup(response.content, 'html.parser', from_encoding='utf-8')

            # å¾®ä¿¡æ–‡ç« çš„ä¸»è¦å†…å®¹é€šå¸¸åœ¨ #js_content ä¸­
            content_div = soup.find('div', id='js_content')
            if content_div:
                # ç§»é™¤æ‰€æœ‰ script å’Œ style æ ‡ç­¾
                for script in content_div(["script", "style"]):
                    script.decompose()

                # è·å–çº¯æ–‡æœ¬
                content = content_div.get_text(separator='\n', strip=True)

                # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
                lines = [line.strip() for line in content.split('\n') if line.strip()]
                content = '\n'.join(lines)

                return content
            else:
                # å¦‚æœæ²¡æ‰¾åˆ° #js_contentï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                title_elem = soup.find('h1', class_='rich_media_title') or soup.find('h1')
                content_elem = soup.find('div', class_='rich_media_content') or soup.find('div', class_='content')

                if content_elem:
                    content = content_elem.get_text(separator='\n', strip=True)
                    lines = [line.strip() for line in content.split('\n') if line.strip()]
                    return '\n'.join(lines)
                else:
                    return "æ— æ³•æå–æ–‡ç« å†…å®¹ï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²æ”¹å˜"

        except requests.exceptions.RequestException as e:
            print(f"  è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
            else:
                return f"è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}"
        except Exception as e:
            print(f"  è§£æå¤±è´¥ (å°è¯• {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
            else:
                return f"è§£ææ–‡ç« å†…å®¹å¤±è´¥: {e}"


def save_article_to_file(title, content, url, date, save_dir="articles"):
    """å°†æ–‡ç« å†…å®¹ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶"""
    # åˆ›å»ºä¿å­˜ç›®å½•
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
    safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).strip()
    if not safe_title:
        safe_title = "untitled"

    # é™åˆ¶æ–‡ä»¶åé•¿åº¦
    if len(safe_title) > 50:
        safe_title = safe_title[:50]

    # æ·»åŠ æ—¶é—´æˆ³é¿å…æ–‡ä»¶åé‡å¤
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{timestamp}_{safe_title}.txt"
    filepath = os.path.join(save_dir, filename)

    # å‡†å¤‡æ–‡ä»¶å†…å®¹
    file_content = f"æ ‡é¢˜: {title}\n"
    file_content += f"é“¾æ¥: {url}\n"
    file_content += f"æ—¶é—´: {date}\n"
    file_content += f"ä¿å­˜æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    file_content += "=" * 80 + "\n\n"
    file_content += content

    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(file_content)
        print(f"  âœ… æ–‡ç« å·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    except Exception as e:
        print(f"  âŒ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
        return None


def save_summary_to_file(articles, save_dir="articles"):
    """ä¿å­˜æ–‡ç« æ‘˜è¦åˆ°æ±‡æ€»æ–‡ä»¶"""
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    summary_file = os.path.join(save_dir, f"articles_summary_{timestamp}.txt")

    summary_content = f"å¾®ä¿¡æ–‡ç« æ±‡æ€»\n"
    summary_content += f"ä¿å­˜æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    summary_content += f"æ–‡ç« æ€»æ•°: {len(articles)}\n"
    summary_content += "=" * 80 + "\n\n"

    for i, article in enumerate(articles, 1):
        summary_content += f"{i}. {article['title']}\n"
        summary_content += f"   é“¾æ¥: {article['url']}\n"
        summary_content += f"   æ—¶é—´: {article['date']}\n"
        summary_content += f"   æ–‡ä»¶: {article.get('filename', 'æœªä¿å­˜')}\n"
        summary_content += "-" * 40 + "\n"

    try:
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        print(f"\nğŸ“‹ æ–‡ç« æ±‡æ€»å·²ä¿å­˜åˆ°: {summary_file}")
        return summary_file
    except Exception as e:
        print(f"âŒ ä¿å­˜æ±‡æ€»å¤±è´¥: {e}")
        return None


# è¿™é‡Œçš„ URL æ˜¯æ ¹æ® WeWe RSS çš„æ ‡å‡†æ¥å£æ‹¼æ¥çš„
# å¦‚æœåªæƒ³è·å–ç‰¹å®šå…¬ä¼—å·ï¼Œå¯ä»¥å°† all æ”¹ä¸ºå¯¹åº”çš„ ID
url = "http://47.99.87.139:4000/feeds/all.json"

try:
    print(f"æ­£åœ¨è¯·æ±‚: {url} ...")
    response = requests.get(url, timeout=10)
    response.raise_for_status() # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ

    data = response.json()

    # æ£€æŸ¥æ˜¯å¦æœ‰ items å­—æ®µ
    if 'items' in data:
        items = data['items']
        print(f"æˆåŠŸè·å–åˆ° {len(items)} ç¯‡æ–‡ç« ï¼š\n")

        # é™åˆ¶è·å–å‰5ç¯‡æ–‡ç« çš„å†…å®¹ä½œä¸ºæµ‹è¯•
        max_articles = min(5, len(items))
        saved_articles = []

        print(f"å‡†å¤‡è·å–å‰ {max_articles} ç¯‡æ–‡ç« çš„å†…å®¹å¹¶ä¿å­˜åˆ°æœ¬åœ°...")

        for i, item in enumerate(items[:max_articles]):
            title = item.get('title', 'æ— æ ‡é¢˜')
            # é“¾æ¥é€šå¸¸åœ¨ url æˆ– id å­—æ®µä¸­
            link = item.get('url') or item.get('id')
            date = item.get('date_published', '')

            print(f"\n{'='*60}")
            print(f"æ–‡ç«  {i+1}/{max_articles}")
            print(f"æ ‡é¢˜: {title}")
            print(f"é“¾æ¥: {link}")
            print(f"æ—¶é—´: {date}")
            print(f"{'='*60}")

            article_info = {
                'title': title,
                'url': link,
                'date': date,
                'content': None,
                'filename': None
            }

            if link and link.startswith('http'):
                # è·å–æ–‡ç« å†…å®¹
                content = get_article_content(link)
                article_info['content'] = content

                if content and not content.startswith("è·å–æ–‡ç« å†…å®¹å¤±è´¥") and not content.startswith("è§£ææ–‡ç« å†…å®¹å¤±è´¥"):
                    # ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶
                    filename = save_article_to_file(title, content, link, date)
                    article_info['filename'] = filename if filename else "ä¿å­˜å¤±è´¥"

                    # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                    print(f"\nğŸ“ æ–‡ç« å†…å®¹é¢„è§ˆ:")
                    print("-" * 40)
                    preview = content[:800] + "..." if len(content) > 800 else content
                    print(preview)
                    print("-" * 40)
                    print(f"ğŸ“Š å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                else:
                    print(f"âŒ è·å–å†…å®¹å¤±è´¥: {content}")
                    article_info['filename'] = "è·å–å¤±è´¥"
            else:
                print("âŒ æ— æ•ˆçš„é“¾æ¥")
                article_info['filename'] = "æ— æ•ˆé“¾æ¥"

            saved_articles.append(article_info)

            # åœ¨æ–‡ç« ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i < max_articles - 1:
                print("\nâ³ ç­‰å¾…3ç§’åå¤„ç†ä¸‹ä¸€ç¯‡æ–‡ç« ...")
                time.sleep(3)

        # ä¿å­˜æ–‡ç« æ±‡æ€»
        print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼å·²å¤„ç†å‰ {max_articles} ç¯‡æ–‡ç« ã€‚")
        save_summary_to_file(saved_articles)

        # ç»Ÿè®¡ä¿¡æ¯
        success_count = sum(1 for article in saved_articles if article['filename'] and 'å¤±è´¥' not in article['filename'])
        print(f"\nğŸ“ˆ ä¿å­˜ç»Ÿè®¡:")
        print(f"   æˆåŠŸä¿å­˜: {success_count}/{max_articles} ç¯‡")
        print(f"   ä¿å­˜ç›®å½•: articles/")
        print(f"   å¦‚éœ€è·å–å…¨éƒ¨ {len(items)} ç¯‡æ–‡ç« ï¼Œè¯·ä¿®æ”¹è„šæœ¬ä¸­çš„ max_articles å˜é‡ã€‚")
    else:
        print("æœªåœ¨è¿”å›æ•°æ®ä¸­æ‰¾åˆ°æ–‡ç« åˆ—è¡¨ã€‚")

except requests.exceptions.RequestException as e:
    print(f"è¯·æ±‚å¤±è´¥: {e}")
    print("å»ºè®®ï¼šå¦‚æœè¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥è¯¥æœåŠ¡å™¨æ˜¯å¦å…è®¸å¤–éƒ¨è®¿é—®ï¼Œæˆ–å°è¯•åœ¨æµè§ˆå™¨ä¸­ç›´æ¥æ‰“å¼€ .json é“¾æ¥ã€‚")
except json.JSONDecodeError:
    print("è§£æ JSON å¤±è´¥ï¼Œè¿”å›çš„å†…å®¹å¯èƒ½ä¸æ˜¯ JSON æ ¼å¼ã€‚")
