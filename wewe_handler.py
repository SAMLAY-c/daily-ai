# wewe_handler.py
import requests
import json
import os
import time
from bs4 import BeautifulSoup
from datetime import datetime

class WeWeHandler:
    def __init__(self):
        self.rss_url = os.getenv("WEWE_RSS_URL")
        self.history_file = "wewe_history.json"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        }
        self.load_history()

    def load_history(self):
        if os.path.exists(self.history_file):
            with open(self.history_file, 'r', encoding='utf-8') as f:
                self.history = json.load(f)
        else:
            self.history = []

    def save_history(self):
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(self.history, f, ensure_ascii=False, indent=2)

    def is_processed(self, url):
        return url in self.history

    def mark_processed(self, url):
        self.history.append(url)
        # ä¿æŒå†å²è®°å½•åœ¨ä¸€å®šå¤§å°ï¼Œé˜²æ­¢æ— é™è†¨èƒ€
        if len(self.history) > 1000:
            self.history = self.history[-1000:]
        self.save_history()

    def fetch_article_list(self):
        """è·å–æ–‡ç« åˆ—è¡¨"""
        if not self.rss_url:
            print("âŒ æœªè®¾ç½® WEWE_RSS_URL")
            return []

        try:
            print(f"ğŸ“¡ æ­£åœ¨è¯·æ±‚ WeWe RSS: {self.rss_url} ...")
            response = requests.get(self.rss_url, timeout=15)
            response.raise_for_status()
            data = response.json()

            items = data.get('items', [])
            new_items = []

            for item in items:
                url = item.get('url') or item.get('id')
                if url and not self.is_processed(url):
                    new_items.append({
                        'title': item.get('title'),
                        'url': url,
                        'date': item.get('date_published', ''),
                        'id': item.get('id')
                    })

            print(f"ğŸ” å‘ç° {len(items)} ç¯‡æ–‡ç« ï¼Œå…¶ä¸­ {len(new_items)} ç¯‡ä¸ºæ–°æ–‡ç« ")
            return new_items

        except Exception as e:
            print(f"âŒ è·å– RSS åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def get_article_content(self, url, max_retries=3):
        """è·å–å¹¶æ¸…æ´—æ–‡ç« å†…å®¹"""
        for attempt in range(max_retries):
            try:
                response = requests.get(url, headers=self.headers, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'html.parser', from_encoding='utf-8')

                # ç§»é™¤å¹²æ‰°å…ƒç´ 
                for script in soup(["script", "style", "iframe", "nav", "footer"]):
                    script.decompose()

                # ä¼˜å…ˆæŸ¥æ‰¾å¾®ä¿¡æ­£æ–‡åŒºåŸŸ
                content_div = soup.find('div', id='js_content') or \
                              soup.find('div', class_='rich_media_content') or \
                              soup.find('div', class_='content')

                if content_div:
                    text = content_div.get_text(separator='\n', strip=True)
                else:
                    text = soup.get_text(separator='\n', strip=True)

                # ç®€å•çš„æ–‡æœ¬æ¸…æ´—
                lines = [line.strip() for line in text.split('\n') if line.strip()]
                return '\n'.join(lines)

            except Exception as e:
                print(f"  âš ï¸ è·å–å†…å®¹é‡è¯• ({attempt+1}/{max_retries}): {e}")
                time.sleep(2)

        return None